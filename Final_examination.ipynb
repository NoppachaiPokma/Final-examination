{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbinteam/010723305/blob/main/Homework9p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOBSUACuAO46",
        "outputId": "505ef385-a17e-4576-ab13-215036eeb504"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "import scipy\n",
        "import matplotlib.pylab as plt\n",
        "from skimage import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-o2dUFY6AO5X"
      },
      "outputs": [],
      "source": [
        "def preprocessing(img) :\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return (img, img_gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "sift = cv2.SIFT_create()\n",
        "bf = cv2.BFMatcher()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augmented_image(frame,im_src, pts_src, pts_dst):\n",
        "    \n",
        "    # Calculate Homography\n",
        "    h, status = cv2.findHomography(pts_src, pts_dst)\n",
        "\n",
        "    # Warp source image to destination based on homography\n",
        "    warped_image = cv2.warpPerspective(im_src, h, (frame.shape[1],frame.shape[0]))\n",
        "            \n",
        "    # Prepare a mask representing region to copy from the warped image into the original frame.\n",
        "    mask = np.zeros([frame.shape[0], frame.shape[1]], dtype=np.uint8)\n",
        "    cv2.fillConvexPoly(mask, np.int32(pts_dst), (255, 255, 255), cv2.LINE_AA)\n",
        "    im_out = cv2.add(frame, warped_image, mask=cv2.bitwise_not(mask))\n",
        "    im_out = cv2.add(im_out, warped_image)\n",
        "    \n",
        "    # cv2.imshow('augmented', im_out)\n",
        "\n",
        "    markerCorners, markerIds, rejectedCandidates = cv2.aruco.detectMarkers(im_out, AruCo_dict, parameters = AruCo_params)\n",
        "\n",
        "    if len(markerCorners) > 0:\n",
        "            rvecs, tvecs, points = cv2.aruco.estimatePoseSingleMarkers(markerCorners , 0.1, K, dist)\n",
        "            for (rvec, tvec, id, corner) in zip(rvecs, tvecs, markerIds, markerCorners) :\n",
        "                x = tvec[0,0]\n",
        "                y = tvec[0,1]\n",
        "                z = tvec[0,2]\n",
        "                text = \"X : {:.2f}  Y : {:.2f}  Z : {:.2f}\".format(x, y, z)\n",
        "                X = (corner[0,0][0] + corner[0,2][0]) / 2\n",
        "                Y = (corner[0,0][1] + corner[0,2][1]) / 2\n",
        "            return X, Y, text\n",
        "    else:\n",
        "        return 0,0 ,\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kmvt8UU-AO5Z"
      },
      "outputs": [],
      "source": [
        "def feature_object_detection(template_img, template_gray, query_img, query_gray, min_match_number) :\n",
        "\n",
        "    img = cv2.imread('./4x4_1000.png')\n",
        "    im_src_size = img.shape[:2]\n",
        "    # src_points = np.float32([[0,0], [im_src_size[1],0],[im_src_size[1], im_src_size[0]] ,[0, im_src_size[0]] ])\n",
        "    src_points = np.float32([[0,0], [0, im_src_size[0]],[im_src_size[1], im_src_size[0]] ,[im_src_size[1],0] ])\n",
        "\n",
        "    template_kpts, template_desc = sift.detectAndCompute(template_gray, None)\n",
        "    query_kpts, query_desc = sift.detectAndCompute(query_gray, None)\n",
        "    matches = bf.knnMatch(template_desc, query_desc, k=2)\n",
        "    good_matches = list()\n",
        "    good_matches_list = list()\n",
        "\n",
        "    for m, n in matches :\n",
        "        if m.distance < 0.6*n.distance :\n",
        "            good_matches.append(m)\n",
        "            good_matches_list.append([m])\n",
        "    \n",
        "    if len(good_matches) > min_match_number :\n",
        "        src_pts = np.float32([ template_kpts[m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
        "        dst_pts = np.float32([ query_kpts[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
        "\n",
        "        H, inlier_masks = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 10) # H RANSAC\n",
        "        # get the bounding box around template image \n",
        "        h, w = template_img.shape[:2]\n",
        "        template_box = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1,1,2)\n",
        "        transformed_box = cv2.perspectiveTransform(template_box, H)\n",
        "        \n",
        "        X, Y, T = augmented_image(query_img, img, src_points,  transformed_box)\n",
        "        \n",
        "\n",
        "        detected_img = cv2.polylines(query_img, [np.int32(transformed_box)], True, (255,0,0), 3, cv2.LINE_AA)\n",
        "        drawmatch_img = cv2.drawMatchesKnn(template_img, template_kpts, detected_img, query_kpts, good_matches_list, None, flags=2, matchesMask=inlier_masks)\n",
        "        \n",
        "        write_text(detected_img, (X, Y), 20, T)\n",
        "        cv2.imshow('frame',   drawmatch_img )\n",
        "        \n",
        "        return\n",
        "    else :\n",
        "        print('Keypoints not enough')\n",
        "        return\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\3 first semester\\img\\Final-examination./new-camera_params/\n",
            "Camera matrix\n",
            "[[726.47630281   0.         466.2575863 ]\n",
            " [  0.         732.06406134 251.99786765]\n",
            " [  0.           0.           1.        ]]\n",
            "Len distortion\n",
            "[[ 0.02450981  0.03012141  0.00045661  0.01036187 -0.07354295]]\n"
          ]
        }
      ],
      "source": [
        "params_dir = os.getcwd()+'./new-camera_params/'\n",
        "print(params_dir)\n",
        "\n",
        "#load camera parameters\n",
        "K = np.load(params_dir+'K.npy')\n",
        "dist = np.load(params_dir+'dist.npy')\n",
        "\n",
        "print(\"Camera matrix\")\n",
        "print(K)\n",
        "print(\"Len distortion\")\n",
        "print(dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "AruCo_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_1000)\n",
        "AruCo_params = cv2.aruco.DetectorParameters_create()\n",
        "# board = cv2.aruco.GridBoard_create(3, 4, 0.05, 0.0075, AruCo_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_text(img, pose, dy, text) :\n",
        "    x0 = pose[0]\n",
        "    y0 = pose[1]\n",
        "    for i, line in enumerate(text.split('\\n')) :\n",
        "        y = y0 + i*dy\n",
        "        cv2.putText(img, line, np.int32([x0, y]), cv2.FONT_HERSHEY_COMPLEX, 0.75, (50,200,255), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fram_gray_real(frame,frame_gray) :\n",
        "    frame_hsv = cv2.cvtColor(frame,cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    kernel =np.array([[255,255,255],[255,255,255],[255,255,255]],np.uint8)\n",
        "    \n",
        "    \n",
        "\n",
        "    upper_range_red=np.array([125,255,255]) \n",
        "    lower_range_red=np.array([[110,50,50]]) \n",
        "\n",
        "    mask_red = cv2.inRange(frame_hsv,lower_range_red,upper_range_red)\n",
        "    \n",
        "\n",
        "    upper_range_yellow=np.array([98,255,255]) \n",
        "    lower_range_yellow=np.array([[90,80,80]]) \n",
        "\n",
        "    mask_yellow = cv2.inRange(frame_hsv,lower_range_yellow,upper_range_yellow)\n",
        "\n",
        "\n",
        "    mask = cv2.add( mask_red,mask_yellow)\n",
        "    mask = cv2.erode(mask,kernel,iterations=2)\n",
        "    mask = cv2.dilate(mask,kernel,iterations=8)\n",
        "    mask_indices=np.where(mask==255)\n",
        "\n",
        "    output_image = np.zeros(frame_gray.shape, dtype = \"uint8\")\n",
        "    output_image[mask_indices] = frame_gray[mask_indices] \n",
        "    \n",
        "    # cv2.imshow('frame1', mask)\n",
        "    cv2.imshow('frame2', output_image)\n",
        "    # cv2.imshow('frame4', frame)\n",
        "    \n",
        "    return output_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ifXvs1uLAO5Y",
        "outputId": "e5a4f09f-c836-4afe-bf6d-2146bd32bc11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n",
            "Keypoints not enough\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-u4kjpz2z\\opencv\\modules\\core\\src\\matmul.dispatch.cpp:531: error: (-215:Assertion failed) scn + 1 == m.cols in function 'cv::perspectiveTransform'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20252/3806995034.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mfarneback_dense_optical_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./right_output.avi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20252/3806995034.py\u001b[0m in \u001b[0;36mfarneback_dense_optical_flow\u001b[1;34m(video_device)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# cv2.imshow('frame4',  frame_gray2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mfeature_object_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate_gray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_gray2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[1;31m# cv2.imshow('frame',   frame )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20252/2153845269.py\u001b[0m in \u001b[0;36mfeature_object_detection\u001b[1;34m(template_img, template_gray, query_img, query_gray, min_match_number)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemplate_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtemplate_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtransformed_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperspectiveTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate_box\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maugmented_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_points\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtransformed_box\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-u4kjpz2z\\opencv\\modules\\core\\src\\matmul.dispatch.cpp:531: error: (-215:Assertion failed) scn + 1 == m.cols in function 'cv::perspectiveTransform'\n"
          ]
        }
      ],
      "source": [
        "template_img = cv2.imread('./Template-3.png')\n",
        "template_img, template_gray = preprocessing(template_img)\n",
        "\n",
        "def farneback_dense_optical_flow(video_device) :\n",
        "    cap = cv2.VideoCapture(video_device)\n",
        "\n",
        "    while cap.isOpened() :\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret :\n",
        "            \n",
        "            frame_gray1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            frame_gray2 = fram_gray_real(frame,frame_gray1)\n",
        "\n",
        "            # cv2.imshow('frame4',  frame_gray2)\n",
        "            feature_object_detection(template_img, template_gray, frame, frame_gray2, 10)\n",
        "            # cv2.imshow('frame',   frame )\n",
        "            key = cv2.waitKey(27) & 0xFF\n",
        "            if key == 27 or key == ord('q') :\n",
        "                break\n",
        "        else :\n",
        "            break\n",
        "    \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "farneback_dense_optical_flow('./right_output.avi')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Lecture_9_Image_warping_Panorama.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d2c1b003d941d6539b8b45c9fc54d69830f7eae3024b66a97cb86f0e3d4b7406"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
